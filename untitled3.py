# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fGfQjCQIfZ4muvkBUbFZnauCi2m965b0
"""



!git clone https://github.com/julia2000457/Gender-Prediction.git

!pip install numpy pandas scikit-learn tensorflow

import numpy as np
import cv2
import os
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt

def preprocess_image(image_path, size=(64, 64)):
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    image = cv2.resize(image, size)
    image = image / 255.0
    return image

def load_dataset(dataset_path):
    images = []
    labels = []
    for gender in ["man", "woman"]:
        gender_path = os.path.join(dataset_path, gender)
        label = 0 if gender == "man" else 1
        for image_name in os.listdir(gender_path):
            image_path = os.path.join(gender_path, image_name)
            image = preprocess_image(image_path)
            images.append(image)
            labels.append(label)

    images = np.array(images).reshape(-1, 64, 64, 1)  # Reshape for model
    labels = to_categorical(np.array(labels), num_classes=2)  # One-hot encode labels
    return train_test_split(images, labels, test_size=0.2, random_state=84)
dataset_path = '/content/Gender-Prediction/dataset1/dataset1/train'
X_train, X_test, y_train, y_test = load_dataset(dataset_path)

# Flatten data for compatibility with ML models
X_train_flat = X_train.reshape(X_train.shape[0], -1)
X_test_flat = X_test.reshape(X_test.shape[0], -1)
y_train_flat = np.argmax(y_train, axis=1)
y_test_flat = np.argmax(y_test, axis=1)

# Train SVM Model
svm_model = SVC(kernel='linear')
svm_model.fit(X_train_flat, y_train_flat)
svm_train_accuracy = accuracy_score(y_train_flat, svm_model.predict(X_train_flat))
svm_test_accuracy = accuracy_score(y_test_flat, svm_model.predict(X_test_flat))
print(f"SVM Train Accuracy: {svm_train_accuracy * 100:.2f}%")
print(f"SVM Test Accuracy: {svm_test_accuracy * 100:.2f}%")

param_grid_svm = {'C': [0.1, 1, 10, 100]}
grid_search_svm = GridSearchCV(SVC(kernel='linear'), param_grid_svm, cv=3)  # Reduce CV folds to 3
grid_search_svm.fit(X_train_flat, y_train_flat)
print(f"Best SVM Parameters: {grid_search_svm.best_params_}")
best_svm_model = grid_search_svm.best_estimator_
best_svm_test_accuracy = best_svm_model.score(X_test_flat, y_test_flat)
print(f"Best SVM Test Accuracy: {best_svm_test_accuracy * 100:.2f}%")

# Simplify Random Forest model for quicker execution
rf_model = RandomForestClassifier(n_estimators=50, random_state=84)
rf_model.fit(X_train_flat, y_train_flat)
rf_train_accuracy = accuracy_score(y_train_flat, rf_model.predict(X_train_flat))
rf_test_accuracy = accuracy_score(y_test_flat, rf_model.predict(X_test_flat))
print(f"Random Forest Train Accuracy: {rf_train_accuracy * 100:.2f}%")
print(f"Random Forest Test Accuracy: {rf_test_accuracy * 100:.2f}%")

rf_scores = cross_val_score(rf_model, X_train_flat, y_train_flat, cv=3)  # Reduce CV folds to 3
print(f"Random Forest Cross-Validation Accuracy: {np.mean(rf_scores) * 100:.2f}%")

param_grid_rf = {
    'n_estimators': [10, 50],  # Simplified for quicker tuning
    'max_depth': [10, None],
    'min_samples_split': [2, 5]
}
grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=84), param_grid_rf, cv=3)  # Reduce CV folds to 3
grid_search_rf.fit(X_train_flat, y_train_flat)
print(f"Best Random Forest Parameters: {grid_search_rf.best_params_}")
best_rf_model = grid_search_rf.best_estimator_
best_rf_test_accuracy = best_rf_model.score(X_test_flat, y_test_flat)
print(f"Best Random Forest Test Accuracy: {best_rf_test_accuracy * 100:.2f}%")

import matplotlib.pyplot as plt
import numpy as np

def plot_model_accuracies(model_names, train_accuracies, test_accuracies, best_test_accuracies):
    bar_width = 0.25  # Width of each bar
    index = np.arange(len(model_names))  # Index for the x-axis positions

    plt.figure(figsize=(12, 6))
    plt.bar(index - bar_width, train_accuracies, width=bar_width, color='g', label='Train Accuracy')
    plt.bar(index, test_accuracies, width=bar_width, color='b', label='Test Accuracy')
    plt.bar(index + bar_width, best_test_accuracies, width=bar_width, color='r', label='Best Test Accuracy (Grid Search)')

    plt.xlabel('Model')
    plt.ylabel('Accuracy (%)')
    plt.title('Model Accuracies Comparison')
    plt.xticks(index, model_names)  # Set model names as x-axis labels
    plt.ylim(0, 100)
    plt.yticks(np.arange(0, 101, 10))
    plt.legend()
    plt.grid(True)
    plt.show()

# Example usage:
model_names = ['SVM', 'Random Forest']
train_accuracies = [99.92, 99.94]  # Replace with actual train accuracies
test_accuracies = [79.69, 80.29]   # Replace with actual test accuracies
best_test_accuracies = [79.06, 80.29]  # Replace with actual best test accuracies

plot_model_accuracies(model_names, train_accuracies, test_accuracies, best_test_accuracies)

from sklearn.metrics import precision_recall_curve, average_precision_score

def plot_precision_recall_curve(model, X, y, title):
    y_score = model.decision_function(X) if hasattr(model, "decision_function") else model.predict_proba(X)[:, 1]
    precision, recall, _ = precision_recall_curve(y, y_score)
    avg_precision = average_precision_score(y, y_score)

    plt.figure(figsize=(10, 6))
    plt.plot(recall, precision, color='b', lw=2, label=f'Precision-Recall curve (AP = {avg_precision:.2f})')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(title)
    plt.legend(loc="lower left")
    plt.show()

# Plot Precision-Recall curves
plot_precision_recall_curve(svm_model, X_test_flat, y_test_flat, "SVM Precision-Recall Curve")
plot_precision_recall_curve(rf_model, X_test_flat, y_test_flat, "Random Forest Precision-Recall Curve")